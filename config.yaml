title: Tooth Fairy 3 Challenge 2025 
description: Configuration file for the Tooth Fairy 3 Challenge training script.

# --------------------------------------------------
# General Settings
# --------------------------------------------------
general:
  experiment_name: tooth_fairy_3_challenge_2025 
  config_source: yaml #options: cmd, yaml

# --------------------------------------------------
# Experiment & Logging Settings
# --------------------------------------------------
args:
  comet: True
  print_config: False
  perform_test: False
  debug_data_limit: -1 #samples per datacenter: P, F, S, '-1' - no limit, use all data
  tags: null

# --------------------------------------------------
# Logger and metrics calculation settings
# --------------------------------------------------
  use_tqdm_print: True
  is_log_qualitative_2d: True
  is_log_qualitative_3d: False
  log_slice_2d_interval: 10
  log_3d_scene_training_interval: 20

  run_validation : True
  validation_interval: 20 # it should be divisible by metrics_interval!

  binary_metrics_interval: 20
  distance_metrics_interval: 20
  multiclass_metrics_interval: 20
  multiclass_metrics_firstTime_log: 40
  is_multiclass_one_hot: False
  metrics_device: cuda:1

# --------------------------------------------------
# Device & Parallelism
# --------------------------------------------------
  device: cuda
  visible_devices: 0,1
  cuda_device_id: 0
  parallel: False
  gpu_frac: 0.99
  num_threads: 16
  pin_memory: True
  prefetch_factor: 2

# --------------------------------------------------
# Data & Preprocessing & Augmentation
# --------------------------------------------------
  data: data
  images_folder: imagesTr
  labels_folder: labelsRemapped # Option: labelsTr 
  watershed_maps_folder: deep_watershed_maps/distdir_maps
  cache_dir: /mnt/vdb1/storage/tsz/data/tf3/cache
  experiment_dir: output/comet_nii_logs
  clear_cache: False
  create_preproc_cache: False
  val_items: 10 # validaiton samples per datacenter - 30 scans
  use_json_split: True
  sampler_type: weighted #weighted, random, sequential
  use_thread_loader: False
  num_workers_cache: 16
  num_workers: 8
  use_persistent_dataset: True
  generate_watershed_maps: False
  watershed_maps_source_labels: data/labelsRemapped # Option: data/labelsTr
  keys: 
    - image
    - label
    - watershed_map
  pixdim: 0.3
  houndsfield_clip: 3000
  patch_size: 
    - 288   
    - 288
    - 160
  crop_samples: 1

  # Geometric Augmentation
  use_augmentations: True
  rotation_range_xy: 0.1       # max rotation in radians (~5-6 degrees)
  rotation_range_z: 0.15 
  rotation_prob: 0.75        # probability to apply rotation
  scale_range: 0.05          # skip scaling to avoid interpolation artifacts
  use_anisotropic: True
  min_zoom: 0.9
  max_zoom: 1.1
  zoom_prob: 0.25

  # Intensity Augmentation
  intensity_scale: 0.1
  intensity_scale_prob: 0.75
  intensity_shift: 0.1
  intensity_shift_prob: 0.75
  contrast : 0.2
  intensity_contrast_prob: 0.75

  # Monai & preprocessing
  lazy_interpolation: True

# --------------------------------------------------
# Model Architecture
# --------------------------------------------------
  n_features: 32
  unet_depth: 5
  norm: instance
  activation: relu
  backbone_name: resnet34 #Options: resnet18, resnet34, resnet50
  out_channels: 46 # 10 misc anatomy, 32 teeth, 3 canals + bg
  classes: 45 
  configuration: DIST_DIR_PULP

# --------------------------------------------------
# Training Hyperparameters
# --------------------------------------------------
  epochs: 400
  batch_size: 2
  batch_size_val: 1
  gradient_accumulation: 1
  lr: 1.0e-3
  weight_decay: 1.0e-4
  optimizer: AdamW
  adam_eps: 1.0e-8
  adam_ams: False
  grad_clip: True 
  max_grad_norm: 1.0
  use_scaler: False
  autocast_dtype: float32
  inference_autocast_dtype: float16
  loss_weights: #loss_weights
    multiclass_seg_loss: 0.1
    dist_loss: 10.0
    dir_loss: 1.0
    pulp_loss: 1.0
  nerve_canal_weight: 5.0

# --------------------------------------------------
# Scheduler Settings
# --------------------------------------------------
  scheduler_name: warmup_cosine #Options: cosine_annealing, warmup_cosine, warmup_cosine_restarts
  warmup_steps: 0
  first_cycle_steps: 1.0    
  scheduler_gamma: 0.5
  min_lr: 5.0e-5

# --------------------------------------------------
# Loss & Metrics
# --------------------------------------------------
  seg_loss_name: DiceCELoss #Options 
  focal_gamma: 0.2
  focal_alpha: 0.3
  bce_weight: 1.0e2
  weighting_mode: inverse_frequency_class_weights #Options: none, inverse_frequency_class_weights
  include_background_loss: True
  include_background_metrics: False
  reduction: mean
  loss_names:
    - multiclass_seg_loss
    - dist_loss
    - dir_loss
    - pulp_loss

# --------------------------------------------------
# Checkpointing & Resume
# --------------------------------------------------
  save_checkpoints: True
  checkpoint_dir: checkpoints
  save_interval: 20
  save_optimiser_interval: 80
  save_optimizer: True           
  start_epoch: 1
  trained_model: null
  continue_training: False

# --------------------------------------------------
# Early Stopping
# --------------------------------------------------
  stop_early: False
  patience: 4
  delta: 0.001

# --------------------------------------------------
# Determinism & Reproducibility
# --------------------------------------------------
  seed: 48
  deterministic_algorithms: False